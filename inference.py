# -*- coding: utf-8 -*-
"""efficientnet-b0-pytorch-inference-birdclef-25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EqHe_2zfBnPduY8us2M87jlNXJV7Z4Nd

# **BirdCLEF 2025 Inference Notebook**
This notebook runs inference on BirdCLEF 2025 test soundscapes and generates a submission file. It supports both single model inference and ensemble inference with multiple models. You can find the pre-processing and training processes in the following notebooks:

- [Transforming Audio-to-Mel Spec. | BirdCLEF'25](https://www.kaggle.com/code/kadircandrisolu/transforming-audio-to-mel-spec-birdclef-25)  
- [EfficientNet B0 Pytorch [Train] | BirdCLEF'25](https://www.kaggle.com/code/kadircandrisolu/efficientnet-b0-pytorch-train-birdclef-25)

**Features**
- Audio Preprocessing
- Test-Time Augmentation (TTA)
"""

import os
import gc
import warnings
import logging
import time
import math
import cv2
from pathlib import Path

import numpy as np
import pandas as pd
import librosa
import torch
import torch.nn as nn
import torch.nn.functional as F
import timm
from tqdm.auto import tqdm
from model import BirdCLEFModel
from utils import audio2melspec

warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.ERROR)

class CFG:
    test_soundscapes = os.path.expanduser('~/Dataset/CV/birdclef-2025/test_soundscapes')
    submission_csv = os.path.expanduser('~/Dataset/CV/birdclef-2025/sample_submission.csv')
    taxonomy_csv = os.path.expanduser('~/Dataset/CV/birdclef-2025/taxonomy.csv')
    model_path = os.path.expanduser('~/Dataset/CV/birdclef25-effnetb0-starter-weight')

    pretrained = False
    # Audio parameters
    FS = 32000
    WINDOW_SIZE = 5

    # Mel spectrogram parameters
    N_FFT = 1024
    HOP_LENGTH = 512
    N_MELS = 128
    FMIN = 50
    FMAX = 14000
    TARGET_SHAPE = (256, 256)

    model_name = 'efficientnet_b0'
    in_channels = 1
    device = 'cpu'

    # Inference parameters
    batch_size = 16
    use_tta = False
    tta_count = 3
    threshold = 0.5

    use_specific_folds = False  # If False, use all found models
    folds = [0, 1]  # Used only if use_specific_folds is True

    debug = False
    debug_count = 3

cfg = CFG()

print(f"Using device: {cfg.device}")
print(f"Loading taxonomy data...")
taxonomy_df = pd.read_csv(cfg.taxonomy_csv)
species_ids = taxonomy_df['primary_label'].tolist()
num_classes = len(species_ids)
print(f"Number of classes: {num_classes}")

def process_audio_segment(audio_data, cfg):
    """Process audio segment to get mel spectrogram"""
    if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:
        audio_data = np.pad(audio_data,
                          (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)),
                          mode='constant')

    mel_spec = audio2melspec(audio_data, cfg)

    # Resize if needed
    if mel_spec.shape != cfg.TARGET_SHAPE:
        mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)

    return mel_spec.astype(np.float32)

from numpy.core.multiarray import scalar
import torch
from pathlib import Path
import numpy as np
import librosa

def find_model_files(cfg):
    """Find all .pth model files in the specified model directory"""
    model_dir = Path(cfg.model_path)
    return [str(path) for path in model_dir.glob('**/*.pth')]

def load_models(cfg, num_classes):
    """Load all found model files and prepare them for ensemble"""
    models = []
    model_files = find_model_files(cfg)

    if not model_files:
        print(f"Warning: No model files found under {cfg.model_path}!")
        return models

    print(f"Found a total of {len(model_files)} model files.")

    if cfg.use_specific_folds:
        model_files = [
            f for f in model_files
            for fold in cfg.folds if f"fold{fold}" in f
        ]
        print(f"Using {len(model_files)} model files for the specified folds ({cfg.folds}).")

    for model_path in model_files:
        try:
            print(f"Loading model: {model_path}")
            # Try trusted loading first
            checkpoint = torch.load(model_path, map_location=torch.device(cfg.device), weights_only=False)

        except Exception as e:
            print(f"Warning: Failed to load with weights_only=False: {e}")
            try:
                torch.serialization.add_safe_globals({'scalar': scalar})
                checkpoint = torch.load(model_path, map_location=torch.device(cfg.device), weights_only=True)
                print("Successfully loaded with weights_only=True and safe globals.")
            except Exception as e2:
                print(f"Error loading model {model_path} even with safe globals: {e2}")
                continue

        try:
            model = BirdCLEFModel(cfg)
            model.load_state_dict(checkpoint['model_state_dict'])
            model = model.to(cfg.device)
            model.eval()
            models.append(model)
        except Exception as e:
            print(f"Failed to initialize model from checkpoint: {e}")

    return models

def predict_on_spectrogram(audio_path, models, cfg, species_ids):
    """Process a single audio file and predict species presence for each 5-second segment"""
    predictions = []
    row_ids = []
    soundscape_id = Path(audio_path).stem

    try:
        print(f"Processing {soundscape_id}")
        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)
        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))

        for segment_idx in range(total_segments):
            start = segment_idx * cfg.FS * cfg.WINDOW_SIZE
            end = start + cfg.FS * cfg.WINDOW_SIZE
            segment_audio = audio_data[start:end]
            row_id = f"{soundscape_id}_{(segment_idx + 1) * cfg.WINDOW_SIZE}"
            row_ids.append(row_id)

            mel_spec = process_audio_segment(segment_audio, cfg)

            if cfg.use_tta:
                all_preds = []
                for tta_idx in range(cfg.tta_count):
                    mel_aug = apply_tta(mel_spec, tta_idx)
                    mel_aug_tensor = torch.tensor(mel_aug, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(cfg.device)

                    preds = []
                    for model in models:
                        with torch.no_grad():
                            probs = torch.sigmoid(model(mel_aug_tensor)).cpu().numpy().squeeze()
                            preds.append(probs)
                    all_preds.append(np.mean(preds, axis=0))

                final_preds = np.mean(all_preds, axis=0)
            else:
                mel_tensor = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(cfg.device)
                preds = []
                for model in models:
                    with torch.no_grad():
                        probs = torch.sigmoid(model(mel_tensor)).cpu().numpy().squeeze()
                        preds.append(probs)
                final_preds = np.mean(preds, axis=0)

            predictions.append(final_preds)

    except Exception as e:
        print(f"Error processing {audio_path}: {e}")

    return row_ids, predictions

def apply_tta(spec, tta_idx):
    """Apply test-time augmentation"""
    if tta_idx == 0:
        # Original spectrogram
        return spec
    elif tta_idx == 1:
        # Time shift (horizontal flip)
        return np.flip(spec, axis=1)
    elif tta_idx == 2:
        # Frequency shift (vertical flip)
        return np.flip(spec, axis=0)
    else:
        return spec

def run_inference(cfg, models, species_ids):
    """Run inference on all test soundscapes"""
    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))

    if cfg.debug:
        print(f"Debug mode enabled, using only {cfg.debug_count} files")
        test_files = test_files[:cfg.debug_count]

    print(f"Found {len(test_files)} test soundscapes")

    all_row_ids = []
    all_predictions = []

    for audio_path in tqdm(test_files):
        row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)
        all_row_ids.extend(row_ids)
        all_predictions.extend(predictions)

    return all_row_ids, all_predictions

def create_submission(row_ids, predictions, species_ids, cfg):
    """Create submission dataframe"""
    print("Creating submission dataframe...")

    submission_dict = {'row_id': row_ids}

    for i, species in enumerate(species_ids):
        submission_dict[species] = [pred[i] for pred in predictions]

    submission_df = pd.DataFrame(submission_dict)

    submission_df.set_index('row_id', inplace=True)

    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')

    missing_cols = set(sample_sub.columns) - set(submission_df.columns)
    if missing_cols:
        print(f"Warning: Missing {len(missing_cols)} species columns in submission")
        for col in missing_cols:
            submission_df[col] = 0.0

    submission_df = submission_df[sample_sub.columns]

    submission_df = submission_df.reset_index()

    return submission_df

def main(args=None):
    start_time = time.time()
    print("Starting BirdCLEF-2025 inference...")

    if args is not None:
        cfg.test_soundscapes = args.test_dir
        cfg.submission_csv = args.sample_csv
        cfg.use_tta = args.tta
        output_path = args.output
        checkpoints = args.checkpoints
    else:
        # Fallbacks if not running from CLI
        output_path = "submission.csv"
        checkpoints = None

    print(f"TTA enabled: {cfg.use_tta} (variants: {cfg.tta_count if cfg.use_tta else 0})")

    if checkpoints:
        print(f"Loading {len(checkpoints)} checkpoint(s) from CLI:")
        models = []
        for cp in checkpoints:
            print(f" → {cp}")
            checkpoint = torch.load(cp, map_location=torch.device(cfg.device), weights_only=False)
            model = BirdCLEFModel(cfg, num_classes)
            model.load_state_dict(checkpoint["model_state_dict"])
            model.to(cfg.device).eval()
            models.append(model)
    else:
        models = load_models(cfg, num_classes)

    if not models:
        raise RuntimeError("No models loaded! Check your --checkpoints or cfg.model_path.")

    print(f"Using {'single model' if len(models)==1 else f'ensemble of {len(models)} models'}")

    row_ids, predictions = run_inference(cfg, models, species_ids)
    submission_df = create_submission(row_ids, predictions, species_ids, cfg)
    submission_df.to_csv(output_path, index=False)
    print(f"Saved final submission to {output_path}")

    end_time = time.time()
    print(f"Inference completed in {(end_time - start_time)/60:.2f} minutes")

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="BirdCLEF-25 EfficientNet‑B0 Inference")
    parser.add_argument("--checkpoints", nargs="+", help="List of .pth checkpoint files to load.")
    parser.add_argument("--test_dir", type=str, default=cfg.test_soundscapes, help="Directory with test .ogg soundscapes")
    parser.add_argument("--sample_csv", type=str, default=cfg.submission_csv, help="Path to sample_submission.csv")
    parser.add_argument("--output", type=str, default="submission.csv", help="Output filename for final submission")
    parser.add_argument("--tta", action="store_true", help="Enable TTA (overrides cfg.use_tta)")

    args = parser.parse_args()
    main(args)